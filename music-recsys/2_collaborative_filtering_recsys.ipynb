{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_dataset\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering is one of the most common techniques used for building recommender systems.\n",
    "1. **Find Similar Users**:\n",
    "    - Compute the cosine similarity between users based on their song ratings.\n",
    "    - Recommend users who have the highest cosine similarity scores.\n",
    "\n",
    "2. **Recommend Songs**:\n",
    "    - Predict the ratings a user would give to songs they haven't listened to.\n",
    "    - Recommend songs with the highest predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user-song matrix\n",
    "dataframe = load_dataset()\n",
    "user_song_matrix = dataframe.pivot_table(index='User_Name', columns='Song', values='Star_Rating', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice',\n",
       " 'Bob',\n",
       " 'Charlie',\n",
       " 'David',\n",
       " 'Emily',\n",
       " 'Frank',\n",
       " 'Grace',\n",
       " 'Hannah',\n",
       " 'Ivy',\n",
       " 'Jack',\n",
       " 'Karen',\n",
       " 'Liam',\n",
       " 'Monica',\n",
       " 'Nancy',\n",
       " 'Oliver',\n",
       " 'Paul',\n",
       " 'Quincy',\n",
       " 'Rachel',\n",
       " 'Steve',\n",
       " 'Tom']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = user_song_matrix.index.tolist()\n",
    "users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Users to Follow:\n",
      "{'Emily': 0.3908736874708199, 'Monica': 0.25723820209050213, 'Liam': 0.23947253431888973, 'Karen': 0.20918717073754478, 'Jack': 0.20837327131300545}\n",
      "\n",
      "Songs to Listen To:\n",
      "['Song234', 'Song264', 'Song166', 'Song94', 'Song24', 'Song136', 'Song214', 'Song146', 'Song216', 'Song133']\n"
     ]
    }
   ],
   "source": [
    "def recommender_system_collaborative(user_song_matrix, user_name):\n",
    "    # Compute user-user cosine similarity\n",
    "    user_similarity = cosine_similarity(user_song_matrix)\n",
    "    user_similarity_df = pd.DataFrame(user_similarity, index=user_song_matrix.index, columns=user_song_matrix.index)\n",
    "    \n",
    "    # 1) Recommend similar users\n",
    "    top_users = user_similarity_df[user_name].sort_values(ascending=False)[1:6].to_dict()\n",
    "    \n",
    "    # 2) Recommend songs\n",
    "    # Compute the predicted ratings\n",
    "    user_predictions = np.dot(user_similarity, user_song_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "    user_predictions_df = pd.DataFrame(user_predictions, index=user_song_matrix.index, columns=user_song_matrix.columns)\n",
    "    sorted_user_predictions = user_predictions_df.loc[user_name].sort_values(ascending=False)\n",
    "    \n",
    "    # Filter out songs the user has already rated/listened to\n",
    "    user_data = user_song_matrix.loc[user_name]\n",
    "    already_listened = user_data[user_data > 0].index.tolist()\n",
    "    recommendations = sorted_user_predictions[~sorted_user_predictions.index.isin(already_listened)]\n",
    "    top_song_recommendations = recommendations.head(10).index.tolist()\n",
    "\n",
    "    return top_users, top_song_recommendations, user_similarity_df, user_predictions_df\n",
    "\n",
    "# Use the recommender system for a user (e.g., 'User1')\n",
    "similar_users, recommended_songs, user_similarity_df, _ = recommender_system_collaborative(user_song_matrix, 'Alice')\n",
    "\n",
    "\n",
    "print(\"Similar Users to Follow:\")\n",
    "print(similar_users)\n",
    "print(\"\\nSongs to Listen To:\")\n",
    "print(recommended_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>User_Name</th>\n",
       "      <th>Alice</th>\n",
       "      <th>Bob</th>\n",
       "      <th>Charlie</th>\n",
       "      <th>David</th>\n",
       "      <th>Emily</th>\n",
       "      <th>Frank</th>\n",
       "      <th>Grace</th>\n",
       "      <th>Hannah</th>\n",
       "      <th>Ivy</th>\n",
       "      <th>Jack</th>\n",
       "      <th>Karen</th>\n",
       "      <th>Liam</th>\n",
       "      <th>Monica</th>\n",
       "      <th>Nancy</th>\n",
       "      <th>Oliver</th>\n",
       "      <th>Paul</th>\n",
       "      <th>Quincy</th>\n",
       "      <th>Rachel</th>\n",
       "      <th>Steve</th>\n",
       "      <th>Tom</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390874</td>\n",
       "      <td>0.035852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171132</td>\n",
       "      <td>0.123824</td>\n",
       "      <td>0.208373</td>\n",
       "      <td>0.209187</td>\n",
       "      <td>0.239473</td>\n",
       "      <td>0.257238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108991</td>\n",
       "      <td>0.119319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133466</td>\n",
       "      <td>0.058070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054076</td>\n",
       "      <td>0.124482</td>\n",
       "      <td>0.236298</td>\n",
       "      <td>0.108823</td>\n",
       "      <td>0.117585</td>\n",
       "      <td>0.560933</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charlie</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223390</td>\n",
       "      <td>0.326499</td>\n",
       "      <td>0.322373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231434</td>\n",
       "      <td>0.459809</td>\n",
       "      <td>0.218511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>David</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112675</td>\n",
       "      <td>0.227733</td>\n",
       "      <td>0.467930</td>\n",
       "      <td>0.217362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770631</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily</th>\n",
       "      <td>0.390874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.072663</td>\n",
       "      <td>0.328256</td>\n",
       "      <td>0.151527</td>\n",
       "      <td>0.080624</td>\n",
       "      <td>0.223481</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>0.222492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162226</td>\n",
       "      <td>0.254616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.299856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frank</th>\n",
       "      <td>0.035852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036384</td>\n",
       "      <td>0.101663</td>\n",
       "      <td>0.018136</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.178188</td>\n",
       "      <td>0.185212</td>\n",
       "      <td>0.075741</td>\n",
       "      <td>0.094048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116676</td>\n",
       "      <td>0.144115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grace</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328256</td>\n",
       "      <td>0.036384</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402075</td>\n",
       "      <td>0.211474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hannah</th>\n",
       "      <td>0.171132</td>\n",
       "      <td>0.133466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151527</td>\n",
       "      <td>0.101663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.231103</td>\n",
       "      <td>0.048593</td>\n",
       "      <td>0.248408</td>\n",
       "      <td>0.141479</td>\n",
       "      <td>0.156317</td>\n",
       "      <td>0.083592</td>\n",
       "      <td>0.331384</td>\n",
       "      <td>0.188681</td>\n",
       "      <td>0.177236</td>\n",
       "      <td>0.223650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ivy</th>\n",
       "      <td>0.123824</td>\n",
       "      <td>0.058070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080624</td>\n",
       "      <td>0.018136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.179870</td>\n",
       "      <td>0.098770</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>0.184889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205241</td>\n",
       "      <td>0.222005</td>\n",
       "      <td>0.140652</td>\n",
       "      <td>0.134505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jack</th>\n",
       "      <td>0.208373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223481</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048593</td>\n",
       "      <td>0.179870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.145239</td>\n",
       "      <td>0.127445</td>\n",
       "      <td>0.172526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157199</td>\n",
       "      <td>0.287260</td>\n",
       "      <td>0.154495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karen</th>\n",
       "      <td>0.209187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.459809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>0.178188</td>\n",
       "      <td>0.402075</td>\n",
       "      <td>0.248408</td>\n",
       "      <td>0.098770</td>\n",
       "      <td>0.145239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.255454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177766</td>\n",
       "      <td>0.244115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.395275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Liam</th>\n",
       "      <td>0.239473</td>\n",
       "      <td>0.111847</td>\n",
       "      <td>0.218511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222492</td>\n",
       "      <td>0.185212</td>\n",
       "      <td>0.211474</td>\n",
       "      <td>0.141479</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>0.127445</td>\n",
       "      <td>0.255454</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204691</td>\n",
       "      <td>0.171389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171175</td>\n",
       "      <td>0.205166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monica</th>\n",
       "      <td>0.257238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156317</td>\n",
       "      <td>0.184889</td>\n",
       "      <td>0.172526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223776</td>\n",
       "      <td>0.377188</td>\n",
       "      <td>0.083217</td>\n",
       "      <td>0.196182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nancy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054076</td>\n",
       "      <td>0.186944</td>\n",
       "      <td>0.112675</td>\n",
       "      <td>0.162226</td>\n",
       "      <td>0.094048</td>\n",
       "      <td>0.263828</td>\n",
       "      <td>0.083592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177766</td>\n",
       "      <td>0.171389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033675</td>\n",
       "      <td>0.132367</td>\n",
       "      <td>0.092550</td>\n",
       "      <td>0.048083</td>\n",
       "      <td>0.179902</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oliver</th>\n",
       "      <td>0.183597</td>\n",
       "      <td>0.124482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227733</td>\n",
       "      <td>0.254616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331384</td>\n",
       "      <td>0.205241</td>\n",
       "      <td>0.157199</td>\n",
       "      <td>0.244115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223776</td>\n",
       "      <td>0.033675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455262</td>\n",
       "      <td>0.353480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249243</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paul</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188681</td>\n",
       "      <td>0.222005</td>\n",
       "      <td>0.287260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377188</td>\n",
       "      <td>0.132367</td>\n",
       "      <td>0.455262</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.233126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528723</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quincy</th>\n",
       "      <td>0.108991</td>\n",
       "      <td>0.108823</td>\n",
       "      <td>0.254993</td>\n",
       "      <td>0.217362</td>\n",
       "      <td>0.299856</td>\n",
       "      <td>0.116676</td>\n",
       "      <td>0.230661</td>\n",
       "      <td>0.177236</td>\n",
       "      <td>0.140652</td>\n",
       "      <td>0.154495</td>\n",
       "      <td>0.395275</td>\n",
       "      <td>0.171175</td>\n",
       "      <td>0.083217</td>\n",
       "      <td>0.092550</td>\n",
       "      <td>0.353480</td>\n",
       "      <td>0.233126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219972</td>\n",
       "      <td>0.333746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rachel</th>\n",
       "      <td>0.119319</td>\n",
       "      <td>0.117585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223650</td>\n",
       "      <td>0.134505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205166</td>\n",
       "      <td>0.196182</td>\n",
       "      <td>0.048083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steve</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179902</td>\n",
       "      <td>0.249243</td>\n",
       "      <td>0.528723</td>\n",
       "      <td>0.219972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357409</td>\n",
       "      <td>0.409850</td>\n",
       "      <td>0.229323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "User_Name     Alice       Bob   Charlie     David     Emily     Frank  \\\n",
       "User_Name                                                               \n",
       "Alice      1.000000  0.000000  0.000000  0.000000  0.390874  0.035852   \n",
       "Bob        0.000000  1.000000  0.000000  0.552581  0.000000  0.000000   \n",
       "Charlie    0.000000  0.000000  1.000000  0.000000  0.223390  0.326499   \n",
       "David      0.000000  0.552581  0.000000  1.000000  0.000000  0.000000   \n",
       "Emily      0.390874  0.000000  0.223390  0.000000  1.000000  0.072663   \n",
       "Frank      0.035852  0.000000  0.326499  0.000000  0.072663  1.000000   \n",
       "Grace      0.000000  0.000000  0.322373  0.000000  0.328256  0.036384   \n",
       "Hannah     0.171132  0.133466  0.000000  0.000000  0.151527  0.101663   \n",
       "Ivy        0.123824  0.058070  0.000000  0.000000  0.080624  0.018136   \n",
       "Jack       0.208373  0.000000  0.231434  0.000000  0.223481  0.023623   \n",
       "Karen      0.209187  0.000000  0.459809  0.000000  0.410995  0.178188   \n",
       "Liam       0.239473  0.111847  0.218511  0.000000  0.222492  0.185212   \n",
       "Monica     0.257238  0.000000  0.000000  0.000000  0.000000  0.075741   \n",
       "Nancy      0.000000  0.054076  0.186944  0.112675  0.162226  0.094048   \n",
       "Oliver     0.183597  0.124482  0.000000  0.227733  0.254616  0.000000   \n",
       "Paul       0.000000  0.236298  0.000000  0.467930  0.000000  0.000000   \n",
       "Quincy     0.108991  0.108823  0.254993  0.217362  0.299856  0.116676   \n",
       "Rachel     0.119319  0.117585  0.000000  0.000000  0.000000  0.144115   \n",
       "Steve      0.000000  0.560933  0.000000  0.770631  0.000000  0.000000   \n",
       "Tom        0.000000  0.000000  0.466016  0.000000  0.000000  0.191028   \n",
       "\n",
       "User_Name     Grace    Hannah       Ivy      Jack     Karen      Liam  \\\n",
       "User_Name                                                               \n",
       "Alice      0.000000  0.171132  0.123824  0.208373  0.209187  0.239473   \n",
       "Bob        0.000000  0.133466  0.058070  0.000000  0.000000  0.111847   \n",
       "Charlie    0.322373  0.000000  0.000000  0.231434  0.459809  0.218511   \n",
       "David      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Emily      0.328256  0.151527  0.080624  0.223481  0.410995  0.222492   \n",
       "Frank      0.036384  0.101663  0.018136  0.023623  0.178188  0.185212   \n",
       "Grace      1.000000  0.000000  0.000000  0.000000  0.402075  0.211474   \n",
       "Hannah     0.000000  1.000000  0.231103  0.048593  0.248408  0.141479   \n",
       "Ivy        0.000000  0.231103  1.000000  0.179870  0.098770  0.152590   \n",
       "Jack       0.000000  0.048593  0.179870  1.000000  0.145239  0.127445   \n",
       "Karen      0.402075  0.248408  0.098770  0.145239  1.000000  0.255454   \n",
       "Liam       0.211474  0.141479  0.152590  0.127445  0.255454  1.000000   \n",
       "Monica     0.000000  0.156317  0.184889  0.172526  0.000000  0.204691   \n",
       "Nancy      0.263828  0.083592  0.000000  0.000000  0.177766  0.171389   \n",
       "Oliver     0.000000  0.331384  0.205241  0.157199  0.244115  0.000000   \n",
       "Paul       0.000000  0.188681  0.222005  0.287260  0.000000  0.000000   \n",
       "Quincy     0.230661  0.177236  0.140652  0.154495  0.395275  0.171175   \n",
       "Rachel     0.000000  0.223650  0.134505  0.000000  0.000000  0.205166   \n",
       "Steve      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Tom        0.000000  0.000000  0.000000  0.357409  0.409850  0.229323   \n",
       "\n",
       "User_Name    Monica     Nancy    Oliver      Paul    Quincy    Rachel  \\\n",
       "User_Name                                                               \n",
       "Alice      0.257238  0.000000  0.183597  0.000000  0.108991  0.119319   \n",
       "Bob        0.000000  0.054076  0.124482  0.236298  0.108823  0.117585   \n",
       "Charlie    0.000000  0.186944  0.000000  0.000000  0.254993  0.000000   \n",
       "David      0.000000  0.112675  0.227733  0.467930  0.217362  0.000000   \n",
       "Emily      0.000000  0.162226  0.254616  0.000000  0.299856  0.000000   \n",
       "Frank      0.075741  0.094048  0.000000  0.000000  0.116676  0.144115   \n",
       "Grace      0.000000  0.263828  0.000000  0.000000  0.230661  0.000000   \n",
       "Hannah     0.156317  0.083592  0.331384  0.188681  0.177236  0.223650   \n",
       "Ivy        0.184889  0.000000  0.205241  0.222005  0.140652  0.134505   \n",
       "Jack       0.172526  0.000000  0.157199  0.287260  0.154495  0.000000   \n",
       "Karen      0.000000  0.177766  0.244115  0.000000  0.395275  0.000000   \n",
       "Liam       0.204691  0.171389  0.000000  0.000000  0.171175  0.205166   \n",
       "Monica     1.000000  0.000000  0.223776  0.377188  0.083217  0.196182   \n",
       "Nancy      0.000000  1.000000  0.033675  0.132367  0.092550  0.048083   \n",
       "Oliver     0.223776  0.033675  1.000000  0.455262  0.353480  0.000000   \n",
       "Paul       0.377188  0.132367  0.455262  1.000000  0.233126  0.000000   \n",
       "Quincy     0.083217  0.092550  0.353480  0.233126  1.000000  0.000000   \n",
       "Rachel     0.196182  0.048083  0.000000  0.000000  0.000000  1.000000   \n",
       "Steve      0.000000  0.179902  0.249243  0.528723  0.219972  0.000000   \n",
       "Tom        0.000000  0.000000  0.000000  0.000000  0.333746  0.000000   \n",
       "\n",
       "User_Name     Steve       Tom  \n",
       "User_Name                      \n",
       "Alice      0.000000  0.000000  \n",
       "Bob        0.560933  0.000000  \n",
       "Charlie    0.000000  0.466016  \n",
       "David      0.770631  0.000000  \n",
       "Emily      0.000000  0.000000  \n",
       "Frank      0.000000  0.191028  \n",
       "Grace      0.000000  0.000000  \n",
       "Hannah     0.000000  0.000000  \n",
       "Ivy        0.000000  0.000000  \n",
       "Jack       0.000000  0.357409  \n",
       "Karen      0.000000  0.409850  \n",
       "Liam       0.000000  0.229323  \n",
       "Monica     0.000000  0.000000  \n",
       "Nancy      0.179902  0.000000  \n",
       "Oliver     0.249243  0.000000  \n",
       "Paul       0.528723  0.000000  \n",
       "Quincy     0.219972  0.333746  \n",
       "Rachel     0.000000  0.000000  \n",
       "Steve      1.000000  0.000000  \n",
       "Tom        0.000000  1.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". **More Data**: As with many machine learning tasks, having more data can help in making more accurate recommendations. This includes more user interaction data, richer metadata about items, and user profiles.\n",
    "\n",
    ". **Data Quality**: Ensure that the data being used is clean, relevant, and free of biases or errors. Removing outliers or irrelevant data can improve the performance of recommendation algorithms.\n",
    "\n",
    ". **Feature Engineering**: Extract more relevant features from the existing data. For instance, for song recommendations, features like tempo, lyrics sentiment, or time of listening could be useful.\n",
    "\n",
    ". **Diversity in Recommendations**: Ensure that the recommendations aren't just limited to popular items. Introduce diversity in the results so users get exposed to a wider array of choices. there are different ways to introduce diversity\n",
    "\n",
    ". **Hybrid Models**: Instead of relying solely on content-based or collaborative filtering, use a combination. Hybrid models can leverage the strengths of both methods to provide more accurate recommendations.\n",
    "\n",
    ". **Consider Context**: Recommendations can be context-dependent. For instance, a song recommendation for a user might change depending on whether they're at the gym or relaxing at home.\n",
    "\n",
    ". **Feedback Loop**: Allow users to give feedback on the recommendations. This can be used to fine-tune the recommendation algorithm.\n",
    "\n",
    ". **Cold Start Problem**: Address the cold start problem, where new users or items without sufficient interaction history can be challenging for recommendation systems. Techniques like content-based filtering or hybrid models can be useful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Offline Evaluation:\n",
    "   - **Precision and Recall**: These measure how many of the recommended items are relevant, and how many relevant items are recommended, respectively.\n",
    "   - **Mean Average Precision (MAP)**: A popular metric in ranking tasks that considers the order of recommendations.\n",
    "   - **Normalized Discounted Cumulative Gain (NDCG)**: Measures the quality of the ranking of the recommended items.\n",
    "   - **Root Mean Squared Error (RMSE)**: For rating predictions, the RMSE can measure how far the predicted ratings are from the actual ratings.\n",
    "   - **Diversity**: These metrics can help to measure how diverse, new, or surprising the recommendations are, depending on the specific goals of the recommendation system.\n",
    "\n",
    "### 2. Online Evaluation:\n",
    "   - **A/B Testing**: Split users into different groups and provide different versions of recommendations to each group. Compare metrics like click-through rate, conversion rate, time spent on the platform, etc.\n",
    "   - **Multi-Armed Bandit Testing**: An extension of A/B testing that dynamically adjusts the proportion of users seeing each version based on ongoing results.\n",
    "\n",
    "### 3. User Studies:\n",
    "   - **User Surveys and Interviews**: Sometimes, qualitative feedback can provide insights that quantitative metrics miss. Asking users directly how they feel about the recommendations can provide valuable information.\n",
    "   - **User Engagement**: Track how users are interacting with the recommended items. Are they clicking on them, spending time with them, purchasing them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-Item Collaborative Filtering Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item-Item Collaborative Filtering recommends items based on their similarity to items the user has shown preference for. \n",
    "\n",
    "1. **Compute Item Similarity**:\n",
    "    - For each pair of items (in our case, songs), compute their similarity. Common metrics include the Pearson correlation or cosine similarity.\n",
    "    - This results in an item-item similarity matrix.\n",
    "\n",
    "2. **Predict User Ratings**:\n",
    "    - For items the user hasn't interacted with, predict their rating by taking a weighted sum of the user's ratings of other items, where the weights are the similarities of those items to the item in question.\n",
    "\n",
    "3. **Recommend Items**:\n",
    "    - Recommend items that have the highest predicted ratings for the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Song13', 'Song243', 'Song93', 'Song73', 'Song103']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def item_item_recommendation(user_song_matrix, user_name, n_recommendations=5):\n",
    "    # Compute the item-item similarity matrix\n",
    "    item_similarity = pd.DataFrame(cosine_similarity(user_song_matrix.T), \n",
    "                                   index=user_song_matrix.columns, \n",
    "                                   columns=user_song_matrix.columns)\n",
    "    \n",
    "    # Get the user's data\n",
    "    user_data = user_song_matrix.loc[user_name]\n",
    "    \n",
    "    # Predict ratings for items the user hasn't interacted with yet\n",
    "    missing_items = user_data[user_data == 0].index\n",
    "    item_scores = {}\n",
    "    for item in missing_items:\n",
    "        similar_items = item_similarity[item]\n",
    "        predicted_score = sum(user_data * similar_items) / sum(abs(similar_items))\n",
    "        item_scores[item] = predicted_score\n",
    "\n",
    "    # Sort by predicted score and get top n_recommendations\n",
    "    recommended_items = sorted(item_scores, key=item_scores.get, reverse=True)[:n_recommendations]\n",
    "    \n",
    "    return recommended_items\n",
    "\n",
    "# Test the function\n",
    "recommended_songs = item_item_recommendation(user_song_matrix, 'Alice')\n",
    "print(recommended_songs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
